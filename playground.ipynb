{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n",
      "I0309 07:44:05.957231 139905671493440 file_utils.py:38] PyTorch version 1.4.0 available.\n",
      "I0309 07:44:07.491601 139905671493440 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I0309 07:44:07.590372 139905671493440 modeling_bert.py:244] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[nltk_data] Downloading package wordnet to /home/zhouyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zhouyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['TORCH_HOME'] = \"/disks/sdb/torch_home\"\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "from awesome_glue.task import *\n",
    "from awesome_glue.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:59: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.tokenizers import SpacyTokenizer, CharacterTokenizer\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "tf = TextField(SpacyTokenizer().tokenize(\"hello world, hello everyone\"), \n",
    "               {\"spp\": SingleIdTokenIndexer(), \n",
    "                \"chr\": TokenCharactersIndexer()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a1062ec041417892fdc65dafe9920d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a097a8fdc25411284345885c6b6c760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0309 09:58:03.650615 139905671493440 vocabulary.py:258] Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da62b13499494d669c58e430b7c1c5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer, PretrainedTransformerIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "torch.manual_seed(1)\n",
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one sentence per line, like\n",
    "\n",
    "        The###DET dog###NN ate###V the###DET apple###NN\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "\n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                pairs = line.strip().split()\n",
    "                sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n",
    "                yield self.text_to_instance([Token(word) for word in sentence], tags)\n",
    "class LstmTagger(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "    \n",
    "# \"tfm\": PretrainedTransformerIndexer(\"bert-base-uncased\")\n",
    "reader = PosDatasetReader({\"sps\": SingleIdTokenIndexer(), \"chr\": TokenCharactersIndexer(), })\n",
    "train_dataset = reader.read(cached_path(\n",
    "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "    '/master/tutorials/tagger/training.txt'))\n",
    "validation_dataset = reader.read(cached_path(\n",
    "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "    '/master/tutorials/tagger/validation.txt'))\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
    "train_dataset.index_with(vocab)\n",
    "validation_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Everybody, read, that, book]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sps___tokens': 4,\n",
       " 'chr___token_characters': 4,\n",
       " 'chr___num_token_characters': 9}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = train_dataset[1]['sentence']\n",
    "print(tf.tokens)\n",
    "tf.get_padding_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': {'sps': {'tokens': tensor([ 7,  8,  9, 10])},\n",
       "  'chr': {'token_characters': tensor([[15, 16,  2,  9, 10, 11,  3,  7, 10],\n",
       "           [ 9,  2,  4,  7,  0,  0,  0,  0,  0],\n",
       "           [ 5,  6,  4,  5,  0,  0,  0,  0,  0],\n",
       "           [11,  3,  3, 17,  0,  0,  0,  0,  0]])}},\n",
       " 'labels': tensor([0, 2, 1, 0])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = tf.as_tensor(tf.get_padding_lengths())\n",
    "train_dataset[1].as_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for TOY-spacy.data exists\n",
      "*** load TOY-spacy.data from cache cost 3.53 seconds\n"
     ]
    }
   ],
   "source": [
    "toy_data = load_data('TOY', 'spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data =  toy_data['data'][1]\n",
    "dev_data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = toy_data['data'][1]\n",
    "from allennlp.data.dataset_readers.dataset_reader import AllennlpDataset\n",
    "ds = AllennlpDataset(val_data, toy_data['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokens_to_indices() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-17f5ef4075fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/allennlp/data/fields/text_field.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, vocab)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mpotentially\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mseveral\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTokenIndexers\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mgets\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmax\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mover\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0massociated\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0meach\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthese\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             raise ConfigurationError(\n",
      "\u001b[0;31mTypeError\u001b[0m: tokens_to_indices() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "ds.instances[5].fields['sent'].index(ds.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for SST-spacy.data exists\n",
      "*** load SST-spacy.data from cache cost 3.36 seconds\n",
      "cache for SST-glove.vec exists\n",
      "*** load SST-glove.vec from cache cost 0.0123 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Basic Args:\n",
       "\t--task_id=SST\n",
       "\t--finetunable=True\n",
       "\t--arch=lstm\n",
       "\t--pretrain=glove\n",
       "\t--_model_name=\n",
       "\t--mode=transfer\n",
       "\t--adv_data=nogit/SST-lstm.hotflip.adv.tsv\n",
       "\t--transform=embed_aug\n",
       "\t--randomness=False\n",
       "\t--aug_data=\n",
       "\t--attack_method=hotflip\n",
       "\t--attack_vectors=counter\n",
       "\t--attack_data_split=dev\n",
       "\t--attack_size=400\n",
       "\t--attack_gen_aug=False\n",
       "\t--attack_gen_adv=True\n",
       "\t--alchemist=False\n",
       "\t--seed=2\n",
       "Deduced Args:\n",
       "\t--model_name=SST-lstm\n",
       "\t--tokenizer=spacy"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "task = Task(config)\n",
    "task.from_pretrained()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for SST-counter.vec exists\n",
      "*** load SST-counter.vec from cache cost 0.0135 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 2/50 [00:00<00:19,  2.49it/s]\u001b[A\n",
      "  6%|▌         | 3/50 [00:01<00:20,  2.25it/s]\u001b[A\n",
      "  8%|▊         | 4/50 [00:01<00:16,  2.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales .\n",
      "\t ['  0.0303', '  0.9697']\n",
      "[adv] the acting , costumes , music , cinematography nor sound are all astounding given the production 's austere locales .\n",
      "\t ['  0.8179', '  0.1821']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.0 Avg.change% 5.0\n",
      "Aggregated metric: Accu before: 100.00%, after: 66.67%, Flip ratio 33.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 5/50 [00:02<00:20,  2.17it/s]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:02<00:15,  2.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women .\n",
      "\t ['  0.0014', '  0.9986']\n",
      "[adv] although laced with humor nor a few fanciful touches , the film is a blithely serious look at young women .\n",
      "\t ['  0.6060', '  0.3940']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 1.5 Avg.change% 7.26\n",
      "Aggregated metric: Accu before: 100.00%, after: 60.00%, Flip ratio 40.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 7/50 [00:02<00:15,  2.80it/s]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:03<00:19,  2.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance .\n",
      "\t ['  0.0027', '  0.9973']\n",
      "[adv] tu does n't have to know about music to appreciate the film 's easygoing blend of comedy nor romance .\n",
      "\t ['  0.6786', '  0.3214']\n",
      "[changed] 3\n",
      "\n",
      "Avg.change# 2.0 Avg.change% 9.84\n",
      "Aggregated metric: Accu before: 100.00%, after: 62.50%, Flip ratio 37.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 9/50 [00:04<00:25,  1.61it/s]\u001b[A\n",
      " 20%|██        | 10/50 [00:04<00:24,  1.65it/s]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:05<00:27,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie .\n",
      "\t ['  0.9599', '  0.0401']\n",
      "[adv] it surrounds a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the equal movie .\n",
      "\t ['  0.1465', '  0.8535']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 2.0 Avg.change% 9.05\n",
      "Aggregated metric: Accu before: 100.00%, after: 63.64%, Flip ratio 36.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 12/50 [00:06<00:24,  1.55it/s]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:07<00:19,  1.80it/s]\u001b[A\n",
      " 30%|███       | 15/50 [00:07<00:19,  1.80it/s]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:08<00:19,  1.79it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] the emotions are raw and will strike a nerve with anyone who 's ever had family trauma .\n",
      "\t ['  0.0160', '  0.9840']\n",
      "[adv] the emotions are raw nor will strike a nerve with anyone who 's ever had family trauma .\n",
      "\t ['  0.6089', '  0.3911']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.8 Avg.change% 8.35\n",
      "Aggregated metric: Accu before: 93.75%, after: 62.50%, Flip ratio 33.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▍      | 17/50 [00:09<00:22,  1.49it/s]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:09<00:17,  1.80it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:09<00:16,  1.83it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] in its best moments , resembles a bad high school production of grease , without benefit of song .\n",
      "\t ['  0.9007', '  0.0993']\n",
      "[adv] across its best moments , recalling a bad high school production of grease , without benefit of song .\n",
      "\t ['  0.2009', '  0.7991']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 1.83 Avg.change% 8.71\n",
      "Aggregated metric: Accu before: 94.74%, after: 63.16%, Flip ratio 33.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 21/50 [00:10<00:12,  2.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] the iditarod lasts for days - this just felt like it did .\n",
      "\t ['  0.7717', '  0.2283']\n",
      "[adv] the iditarod expands for days - this just felt like it did .\n",
      "\t ['  0.1683', '  0.8317']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.71 Avg.change% 8.57\n",
      "Aggregated metric: Accu before: 90.48%, after: 57.14%, Flip ratio 36.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 23/50 [00:10<00:10,  2.65it/s]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:11<00:11,  2.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] seldom has a movie so closely matched the spirit of a man and his work .\n",
      "\t ['  0.0113', '  0.9887']\n",
      "[adv] seldom has a movie so closely matched the spirit of a man nor his work .\n",
      "\t ['  0.5523', '  0.4477']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.62 Avg.change% 8.28\n",
      "Aggregated metric: Accu before: 87.50%, after: 54.17%, Flip ratio 38.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 25/50 [00:12<00:13,  1.83it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] nicks , seemingly uncertain what 's going to make people laugh , runs the gamut from stale parody to raunchy sex gags to formula romantic comedy .\n",
      "\t ['  0.9815', '  0.0185']\n",
      "[adv] notches , seemingly uncertain what 's going to deliver people laugh , manages the gamut from stale parody to raunchy sex gags to formula romantic comedy .\n",
      "\t ['  0.4842', '  0.5158']\n",
      "[changed] 3\n",
      "\n",
      "Avg.change# 1.78 Avg.change% 8.59\n",
      "Aggregated metric: Accu before: 88.00%, after: 52.00%, Flip ratio 40.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 26/50 [00:13<00:16,  1.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] the action switches between past and present , but the material link is too tenuous to anchor the emotional connections that purport to span a 125-year divide .\n",
      "\t ['  0.9791', '  0.0209']\n",
      "[adv] the action switches between past and present , nonetheless the material connection is equally tenuous to anchor the emotional connections that purport to span a 125-year divide .\n",
      "\t ['  0.2296', '  0.7704']\n",
      "[changed] 3\n",
      "\n",
      "Avg.change# 1.9 Avg.change% 8.8\n",
      "Aggregated metric: Accu before: 88.46%, after: 50.00%, Flip ratio 43.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 27/50 [00:14<00:16,  1.35it/s]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:14<00:13,  1.58it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] it 's a cookie - cutter movie , a cut - and - paste job .\n",
      "\t ['  0.9668', '  0.0332']\n",
      "[adv] it 's a cookie - cutter movie , a cutting - and - pasta job .\n",
      "\t ['  0.2769', '  0.7231']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 1.91 Avg.change% 9.14\n",
      "Aggregated metric: Accu before: 89.29%, after: 50.00%, Flip ratio 44.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 29/50 [00:14<00:11,  1.83it/s]\u001b[A\n",
      " 60%|██████    | 30/50 [00:15<00:12,  1.56it/s]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:16<00:12,  1.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] ... designed to provide a mix of smiles and tears , ` ` crossroads '' instead provokes a handful of unintentional howlers and numerous yawns .\n",
      "\t ['  0.6131', '  0.3869']\n",
      "[adv] ... designed to provide a mix of smiles and tears , ` ` crossroads '' nonetheless provokes a handful of unintentional howlers and numerous yawns .\n",
      "\t ['  0.0660', '  0.9340']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.83 Avg.change% 8.7\n",
      "Aggregated metric: Accu before: 90.32%, after: 51.61%, Flip ratio 42.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 34/50 [00:16<00:08,  1.88it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:17<00:06,  2.25it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:17<00:04,  2.68it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] as surreal as a dream and as detailed as a photograph , as visually dexterous as it is at times imaginatively overwhelming .\n",
      "\t ['  0.0032', '  0.9968']\n",
      "[adv] as surreal as a sleep nor as detailed as a photograph , as visually dexterous as it is at times imaginatively overwhelming .\n",
      "\t ['  0.6238', '  0.3762']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 1.85 Avg.change% 8.7\n",
      "Aggregated metric: Accu before: 84.21%, after: 50.00%, Flip ratio 40.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 39/50 [00:18<00:04,  2.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] escaping the studio , piccoli is warmly affecting and so is this adroitly minimalist movie .\n",
      "\t ['  0.0178', '  0.9822']\n",
      "[adv] escaping the studio , piccoli is warmly affecting and so is this adroitly negligible movie .\n",
      "\t ['  0.7835', '  0.2165']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.79 Avg.change% 8.52\n",
      "Aggregated metric: Accu before: 84.62%, after: 48.72%, Flip ratio 42.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 40/50 [00:18<00:04,  2.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] there 's ... tremendous energy from the cast , a sense of playfulness and excitement that seems appropriate .\n",
      "\t ['  0.0006', '  0.9994']\n",
      "[adv] there 's ... tremendous energy from the thrown , a sense of playfulness nor excitement that seems appropriate .\n",
      "\t ['  0.7929', '  0.2071']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 1.8 Avg.change% 8.66\n",
      "Aggregated metric: Accu before: 85.00%, after: 47.50%, Flip ratio 44.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 41/50 [00:19<00:04,  1.92it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:20<00:04,  1.80it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] the subtle strength of ` ` elling '' is that it never loses touch with the reality of the grim situation .\n",
      "\t ['  0.0499', '  0.9501']\n",
      "[adv] the subtle endurance of ` ` elling '' is that it never loses touch with the reality of the grim situation .\n",
      "\t ['  0.7557', '  0.2443']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.75 Avg.change% 8.4\n",
      "Aggregated metric: Accu before: 85.71%, after: 47.62%, Flip ratio 44.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 43/50 [00:20<00:03,  2.17it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:21<00:02,  2.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] the title not only describes its main characters , but the lazy people behind the camera as well .\n",
      "\t ['  0.8483', '  0.1517']\n",
      "[adv] the title not only describes its main characters , but the lazy beings behind the camera as well .\n",
      "\t ['  0.3745', '  0.6255']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.71 Avg.change% 8.22\n",
      "Aggregated metric: Accu before: 86.36%, after: 47.73%, Flip ratio 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 45/50 [00:21<00:02,  2.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] it offers little beyond the momentary joys of pretty and weightless intellectual entertainment .\n",
      "\t ['  0.6522', '  0.3478']\n",
      "[adv] it offers tiny beyond the momentary joys of pretty and weightless intellectual entertainment .\n",
      "\t ['  0.0037', '  0.9963']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.67 Avg.change% 8.16\n",
      "Aggregated metric: Accu before: 86.67%, after: 46.67%, Flip ratio 46.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 46/50 [00:21<00:01,  2.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] a synthesis of cliches and absurdities that seems positively decadent in its cinematic flash and emptiness .\n",
      "\t ['  0.9636', '  0.0364']\n",
      "[adv] a synthesis of sociological and absurdities that seems positively decadent in its cinematic wink and emptiness .\n",
      "\t ['  0.4770', '  0.5230']\n",
      "[changed] 2\n",
      "\n",
      "Avg.change# 1.68 Avg.change% 8.35\n",
      "Aggregated metric: Accu before: 86.96%, after: 45.65%, Flip ratio 47.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▍| 47/50 [00:22<00:01,  2.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] a subtle and well - crafted ( for the most part ) chiller .\n",
      "\t ['  0.0015', '  0.9985']\n",
      "[adv] a subtle nor well - crafted ( for the most part ) chiller .\n",
      "\t ['  0.7256', '  0.2744']\n",
      "[changed] 1\n",
      "\n",
      "Avg.change# 1.65 Avg.change% 8.29\n",
      "Aggregated metric: Accu before: 87.23%, after: 44.68%, Flip ratio 48.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 48/50 [00:22<00:00,  2.31it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [00:23<00:00,  1.99it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raw] it feels like an after - school special gussied up with some fancy special effects , and watching its rote plot points connect is about as exciting as gazing at an egg timer for 93 minutes .\n",
      "\t ['  0.9829', '  0.0171']\n",
      "[adv] it discoveries like an after - school unique gussied up with some fancy special effects , and watching its rouge plot points connect is about as exciting as gazing at an egg timer for 93 record .\n",
      "\t ['  0.2827', '  0.7173']\n",
      "[changed] 4\n",
      "\n",
      "Avg.change# 1.76 Avg.change% 8.41\n",
      "Aggregated metric: Accu before: 88.00%, after: 46.00%, Flip ratio 47.73%\n",
      "Avg.change# 1.76 Avg.change% 8.41\n",
      "Accu before: 88.00%, after: 46.00%, Flip ratio 47.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config.attack_size = 50\n",
    "config.attack_method = \"pwws\"\n",
    "task.attack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "stops = stopwords.words(\"english\")\n",
    "all_words = list(task.vocab.get_index_to_token_vocabulary().values())[2:]\n",
    "for word in all_words:\n",
    "    if \"'\" in word:\n",
    "        continue\n",
    "    scores[word] = round(task.predictor.predict(word)['probs'][0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6375"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from awesome_glue.utils import FreqUtil\n",
    "# FreqUtil.split_by_frequency(task.vocab, 19, 20)\n",
    "task.vocab._retained_counter['tokens']['Microsoft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Vocabulary' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-40bddff12b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Vocabulary' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(30):\n",
    "    sent = allenutil.as_sentence(task.dev_data[i])\n",
    "    before = task.predictor.predict(sent)['probs'][0]\n",
    "    if before > 0.5:\n",
    "        sent = \"above above \" + sent\n",
    "    else:\n",
    "        sent = \"too too \" + sent\n",
    "    after = task.predictor.predict(sent)['probs'][0]\n",
    "    if (before - 0.5) * (after - 0.5) < 0:\n",
    "        total += 1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('voices', 0.13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.items(), key=lambda x:x[1], reverse=False)[:5000][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12732"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.vocab.get_token_index('d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
