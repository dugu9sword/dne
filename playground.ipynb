{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0328 01:11:10.679780 139695034586944 file_utils.py:38] PyTorch version 1.4.0 available.\n",
      "I0328 01:11:12.392734 139695034586944 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "Loading faiss with AVX2 support.\n",
      "I0328 01:11:17.668418 139695034586944 modeling_bert.py:244] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['TORCH_HOME'] = \"/disks/sdb/torch_home\"\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "from awesome_glue.task import *\n",
    "from awesome_glue.config import Config\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0328 08:41:00.650466 139695034586944 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0328 08:41:00.652191 139695034586944 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0328 08:41:01.739138 139695034586944 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /disks/sdb/torch_home/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.tokenizers import PretrainedTransformerTokenizer\n",
    "tokenizer = PretrainedTransformerTokenizer(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.tokenize_sentence_pair(\"hello thdde world\", \"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': [101, 7592, 16215, 14141, 2063, 2088, 102, 6616, 102],\n",
       " 'mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "v = Vocabulary()\n",
    "y.tokens_to_indices(x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_token_to_index_vocabulary(\"tags\")['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0328 08:46:42.046995 139695034586944 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0328 08:46:42.048886 139695034586944 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0328 08:46:43.166237 139695034586944 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /disks/sdb/torch_home/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers.pretrained_transformer_indexer import \\\n",
    "    PretrainedTransformerIndexer\n",
    "y = PretrainedTransformerIndexer('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for AGNEWS-spacy.data exists\n",
      "*** load AGNEWS-spacy.data from cache cost 19.4 seconds\n",
      "cache for AGNEWS-glove.vec exists\n",
      "*** load AGNEWS-glove.vec from cache cost 0.111 seconds\n",
      "Load model from saved/models/AGNEWS-lstm/best.th\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Basic Args:\n",
       "\t--task_id=AGNEWS\n",
       "\t--finetunable=True\n",
       "\t--arch=lstm\n",
       "\t--pretrain=glove\n",
       "\t--_model_name=\n",
       "\t--mode=attack\n",
       "\t--aug_data=\n",
       "\t--adv_iter=0\n",
       "\t--adv_policy=hot\n",
       "\t--adv_replace_num=5\n",
       "\t--adv_constraint=True\n",
       "\t--pred_ensemble=1\n",
       "\t--pred_transform=\n",
       "\t--pred_transform_args=\n",
       "\t--attack_method=pwws\n",
       "\t--attack_vectors=counter\n",
       "\t--attack_data_split=dev\n",
       "\t--attack_size=200\n",
       "\t--attack_gen_adv=False\n",
       "\t--adv_data=nogit/AGNEWS-lstm.hotflip.adv.tsv\n",
       "\t--alchemist=False\n",
       "\t--seed=2\n",
       "\t--cuda=0\n",
       "Deduced Args:\n",
       "\t--model_name=AGNEWS-lstm\n",
       "\t--tokenizer=spacy"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(2)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "task = Task(config)\n",
    "task.from_pretrained()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d069a676f5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-n 1000000 torch.topk(a.cuda(), 3)[0].cpu()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2311\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.zeros(19, 19)\n",
    "%timeit -n 1000000 torch.topk(a.cuda(), 3)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for AGNEWS-counter.vec exists\n",
      "*** load AGNEWS-counter.vec from cache cost 0.116 seconds\n"
     ]
    }
   ],
   "source": [
    "_, spacy_vec = task.get_spacy_vocab_and_vec()\n",
    "index = faiss.IndexFlatL2(spacy_vec.shape[1])\n",
    "res = faiss.StandardGpuResources()  # use a single GPU\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "embed =  spacy_vec.cpu().numpy()\n",
    "index.add(embed)\n",
    "_, I = index.search(embed, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I = I.tolist()\n",
    "edges = []\n",
    "for idx in range(len(I)):\n",
    "    if I[idx][0] == 0:\n",
    "        edges.append([idx, idx])\n",
    "        continue\n",
    "    else:\n",
    "        edges.extend([[idx, ele] for ele in I[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "edges = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.3577,  0.0860, -0.2190, -0.3582, -0.1743],\n",
       "        [ 0.4214, -0.8537,  0.0494, -0.5716, -0.6191]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.modules.token_embedders.embedding import Embedding\n",
    "from allennlpx.modules.token_embedders.graph_funcs import MeanPooling\n",
    "import torch\n",
    "meanpool = MeanPooling(5, 5)\n",
    "embed = Embedding(num_embeddings=3, embedding_dim=5)\n",
    "meanpool(embed.weight, torch.tensor([[0, 1], [1, 2]]).t())\n",
    "# embed(torch.tensor([0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " \"'s\",\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'as',\n",
       " 'with',\n",
       " 'an',\n",
       " 'film',\n",
       " 'for',\n",
       " 'its',\n",
       " 'movie',\n",
       " 'this',\n",
       " '`',\n",
       " 'you',\n",
       " 'be',\n",
       " 'but',\n",
       " 'on',\n",
       " 'by',\n",
       " 'more',\n",
       " 'one',\n",
       " '--',\n",
       " 'at',\n",
       " 'than',\n",
       " 'has',\n",
       " 'from',\n",
       " 'about',\n",
       " 'his',\n",
       " 'are',\n",
       " 'so',\n",
       " 'all',\n",
       " 'or',\n",
       " 'have',\n",
       " 'most',\n",
       " 'out',\n",
       " 'story',\n",
       " 'too',\n",
       " 'into',\n",
       " 'up',\n",
       " 'who',\n",
       " 'characters',\n",
       " 'i',\n",
       " 'comedy',\n",
       " 'if',\n",
       " 'just']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forbidden_words = DEFAULT_IGNORE_TOKENS\n",
    "forbidden_words.extend([\n",
    "    line.rstrip('\\n') for line in open(TASK_SPECS['SST']['banned_words'])\n",
    "])\n",
    "# forbidden_words += stopwords.words(\"english\")\n",
    "high_words = FreqUtil.topk_frequency(task.vocab, 50, 'most', forbidden_words)\n",
    "high_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for SST-spacy.data exists\n",
      "*** load SST-spacy.data from cache cost 4.23 seconds\n",
      "cache for SST-glove.vec exists\n",
      "*** load SST-glove.vec from cache cost 0.0137 seconds\n",
      "Load model from saved/models/SST-lstm/best.th\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.task_id = 'SST'\n",
    "task = Task(config)\n",
    "task.from_pretrained()\n",
    "config\n",
    "\n",
    "\n",
    "# values = list(task.vocab.get_index_to_token_vocabulary().values())\n",
    "values = high_words\n",
    "results = defaultdict(lambda: [], {})\n",
    "from allennlp.common.util import lazy_groups_of\n",
    "for group in lazy_groups_of(values, 1024):\n",
    "    result = task.predictor.predict_batch_json([{\"sent\": ele} for ele in group])\n",
    "    for i in range(len(result[0]['probs'])):\n",
    "        results[i].extend([ele['probs'][i] for ele in result])\n",
    "pairs = defaultdict(lambda: [], {})\n",
    "for i in range(len(results)):\n",
    "    pairs[i] = sorted(zip(values, results[i]), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for SST-spacy.data exists\n",
      "*** load SST-spacy.data from cache cost 3.22 seconds\n",
      "cache for SST-glove.vec exists\n",
      "*** load SST-glove.vec from cache cost 0.0091 seconds\n",
      "Load model from saved/models/SST-glstm/best.th\n"
     ]
    }
   ],
   "source": [
    "config._model_name = 'SST-glstm'\n",
    "task = Task(config)\n",
    "task.from_pretrained()\n",
    "config\n",
    "\n",
    "# values = list(task.vocab.get_index_to_token_vocabulary().values())\n",
    "values = high_words\n",
    "results = defaultdict(lambda: [], {})\n",
    "from allennlp.common.util import lazy_groups_of\n",
    "for group in lazy_groups_of(values, 1024):\n",
    "    result = task.predictor.predict_batch_json([{\"sent\": ele} for ele in group])\n",
    "    for i in range(len(result[0]['probs'])):\n",
    "        results[i].extend([ele['probs'][i] for ele in result])\n",
    "pairs2 = defaultdict(lambda: [], {})\n",
    "for i in range(len(results)):\n",
    "    pairs2[i] = sorted(zip(values, results[i]), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], []]\n",
      "[[1], [1]]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "x=[]\n",
    "xs = [x] * 2\n",
    "ys = [deepcopy(ele) for ele in xs]\n",
    "ys[0].append(1)\n",
    "print(ys)\n",
    "ys = deepcopy(xs)\n",
    "ys[0].append(1)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'goto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-02e505c50643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am trapped, please rescue!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'goto'"
     ]
    }
   ],
   "source": [
    "from goto import goto, label\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        for k in range(9):\n",
    "            print(\"I am trapped, please rescue!\")\n",
    "            if k == 2:\n",
    "                goto .breakout # breaking out from a deeply nested loop\n",
    "label .breakout\n",
    "print(\"Freedom!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word          normal    adv\n",
      "----------  --------  -----\n",
      "too             0.96   0.95\n",
      "or              0.82   0.98\n",
      "if              0.76   0.86\n",
      "from            0.71   0.44\n",
      "than            0.66   0.98\n",
      "have            0.6    0.03\n",
      "out             0.58   0.9\n",
      "its             0.57   0.42\n",
      "on              0.56   0.38\n",
      "just            0.56   0.91\n",
      "`               0.54   0.4\n",
      "at              0.53   0.44\n",
      "as              0.53   0.32\n",
      "but             0.53   0.35\n",
      "by              0.51   0.46\n",
      "about           0.48   0.31\n",
      "of              0.47   0.49\n",
      "movie           0.47   0.38\n",
      "one             0.47   0.41\n",
      "that            0.47   0.48\n",
      "for             0.47   0.3\n",
      "a               0.46   0.98\n",
      "so              0.45   0.13\n",
      "be              0.45   0.3\n",
      "most            0.45   0.91\n",
      "the             0.44   0.47\n",
      "more            0.44   0.02\n",
      "into            0.43   0.44\n",
      "with            0.42   0.45\n",
      "in              0.42   0.39\n",
      "his             0.42   0.16\n",
      "it              0.42   0.5\n",
      "all             0.41   0.17\n",
      "story           0.4    0.02\n",
      "an              0.39   0.45\n",
      "to              0.39   0.34\n",
      "this            0.39   0.22\n",
      "who             0.36   0.4\n",
      "'s              0.35   0.27\n",
      "are             0.35   0.83\n",
      "--              0.35   0.51\n",
      "i               0.33   0.14\n",
      "up              0.33   0.14\n",
      "film            0.31   0.94\n",
      "and             0.28   0.99\n",
      "has             0.25   0.19\n",
      "is              0.23   0.03\n",
      "comedy          0.19   0.88\n",
      "characters      0.16   0.58\n",
      "you             0.07   0.01\n",
      "word          normal    adv\n",
      "----------  --------  -----\n",
      "you             0.93   0.99\n",
      "characters      0.84   0.42\n",
      "comedy          0.81   0.12\n",
      "is              0.77   0.97\n",
      "has             0.75   0.81\n",
      "and             0.72   0.01\n",
      "film            0.69   0.06\n",
      "up              0.67   0.86\n",
      "i               0.67   0.86\n",
      "--              0.65   0.49\n",
      "are             0.65   0.17\n",
      "'s              0.65   0.73\n",
      "who             0.64   0.6\n",
      "this            0.61   0.78\n",
      "to              0.61   0.66\n",
      "an              0.61   0.55\n",
      "story           0.6    0.98\n",
      "all             0.59   0.83\n",
      "it              0.58   0.5\n",
      "his             0.58   0.84\n",
      "in              0.58   0.61\n",
      "with            0.58   0.55\n",
      "into            0.57   0.56\n",
      "more            0.56   0.98\n",
      "the             0.56   0.53\n",
      "most            0.55   0.09\n",
      "be              0.55   0.7\n",
      "so              0.55   0.87\n",
      "a               0.54   0.02\n",
      "for             0.53   0.7\n",
      "that            0.53   0.52\n",
      "one             0.53   0.59\n",
      "movie           0.53   0.62\n",
      "of              0.53   0.51\n",
      "about           0.52   0.69\n",
      "by              0.49   0.54\n",
      "but             0.47   0.65\n",
      "as              0.47   0.68\n",
      "at              0.47   0.56\n",
      "`               0.46   0.6\n",
      "just            0.44   0.09\n",
      "on              0.44   0.62\n",
      "its             0.43   0.58\n",
      "out             0.42   0.1\n",
      "have            0.4    0.97\n",
      "than            0.34   0.02\n",
      "from            0.29   0.56\n",
      "if              0.24   0.14\n",
      "or              0.18   0.02\n",
      "too             0.04   0.05\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pairs)):\n",
    "    tab = []\n",
    "    for ele1, score1 in pairs[i]:\n",
    "        for ele2, score2 in pairs2[i]:\n",
    "            if ele1 == ele2:\n",
    "                tab.append((ele1, round(score1, 2),  round(score2, 2)))\n",
    "    print(tabulate(tab, headers=['word', 'normal', 'adv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Normal:  too or if from than have out its on just ` at as but by about of movie one that for a so be most the more into with in\n",
      "Adv:  too or out just have an a ` if is all 's so characters but by it than his i has be on as its one -- for are more\n",
      "1\n",
      "Normal:  you characters comedy is has and film up i -- are 's who this to an story all it his in with into more the most be so a for\n",
      "Adv:  story comedy film up you who into movie most in this from of at that and about to with the more are for -- one its as on be has\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pairs)):\n",
    "    print(i)\n",
    "    print('Normal: ', ' '.join(list(map(lambda x: x[0], pairs[i][:30]))))\n",
    "    print('Adv: ', ' '.join(list(map(lambda x: x[0], pairs2[i][:30]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for TOY-spacy.data exists\n",
      "*** load TOY-spacy.data from cache cost 4.25 seconds\n"
     ]
    }
   ],
   "source": [
    "toy_data = load_data(\"TOY\", \"spacy\")\n",
    "vocab = toy_data['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(probs: List[torchTensor], p=2):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3648, 0.4532, 0.1820],\n",
       "        [0.1417, 0.3008, 0.5575],\n",
       "        [0.3776, 0.3312, 0.2912],\n",
       "        [0.0114, 0.4044, 0.5841],\n",
       "        [0.3235, 0.3792, 0.2974]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.rand(5, 3)\n",
    "probs = probs / probs.sum(1, keepdims=True)\n",
    "\n",
    "probs.max(dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
