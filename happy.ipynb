{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlpx.data.dataset_readers.berty_tsv import BertyTSVReader\n",
    "from allennlpx.data.dataset_readers.spacy_tsv import SpacyTSVReader\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from allennlp.data.iterators.basic_iterator import BasicIterator\n",
    "# PretrainedBertIndexer(pretrained_model='bert-base-uncased',\n",
    "#                                                   do_lowercase=True)\n",
    "# rte_task = RTETask(\"glue_data/RTE\", max_seq_len=512, name='rte', tokenizer_name='bert-base-uncased')\n",
    "from allennlp.data.dataset import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07 10:25:30 PM: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.tokenize(\"[CLS] happy unhappy dislike world beijing university happiness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e6b3dc6ee047ddba04925893cabe0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932e022179ff4e75895f04f8c9ec910a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=276.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = SpacyTSVReader(sent1_col='sentence1', label_col='label', add_cls=True)\n",
    "data = reader.read('glue_data/RTE/dev.tsv')\n",
    "vocab = Vocabulary.from_instances(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sent1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8c64f9666b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/allennlp/data/instance.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# you to use ``add_field`` and supply a vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sent1'"
     ]
    }
   ],
   "source": [
    "data[0]['sent1'].tokens\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07 05:39:41 AM: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "01/07 05:39:41 AM: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3db9cb96161446b8166d073733c9591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07 05:39:42 AM: Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815f3a0f08d04494833800af4e6f94d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=276.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07 05:39:42 AM: Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fccead11a54407bbc9510f6c629d380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=276.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = BertyTSVReader(sent1_col='sentence1', sent2_col='sentence2', label_col='label')\n",
    "data = reader.read('glue_data/RTE/dev.tsv')\n",
    "vocab = Vocabulary.from_instances(data)\n",
    "iterator = BasicIterator(batch_size=2)\n",
    "iterator.index_with(Vocabulary.from_instances(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[yet,\n",
       " ,,\n",
       " we,\n",
       " now,\n",
       " are,\n",
       " discovering,\n",
       " that,\n",
       " antibiotics,\n",
       " are,\n",
       " losing,\n",
       " their,\n",
       " effectiveness,\n",
       " against,\n",
       " illness,\n",
       " .,\n",
       " disease,\n",
       " -,\n",
       " causing,\n",
       " bacteria,\n",
       " are,\n",
       " mu,\n",
       " ##tat,\n",
       " ##ing,\n",
       " faster,\n",
       " than,\n",
       " we,\n",
       " can,\n",
       " come,\n",
       " up,\n",
       " with,\n",
       " new,\n",
       " antibiotics,\n",
       " to,\n",
       " fight,\n",
       " the,\n",
       " new,\n",
       " variations,\n",
       " .,\n",
       " [SEP],\n",
       " bacteria,\n",
       " is,\n",
       " winning,\n",
       " the,\n",
       " war,\n",
       " against,\n",
       " antibiotics,\n",
       " .]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['berty_tokens'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = data[0:3]\n",
    "batch = Batch(instances)\n",
    "batch.index_instances(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vocab = reader._token_indexers['berty_tokens'].vocab\n",
    "bert_vocab['##h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Dataset Statistics----\n",
      "\n",
      "Statistics for berty_tokens.berty_tokens_length:\n",
      "\tLengths: Mean: 119.33333333333333, Standard Dev: 50.33443707399096, Max: 164, Min: 49\n",
      "Statistics for berty_tokens.berty_tokens-offsets_length:\n",
      "\tLengths: Mean: 117.33333333333333, Standard Dev: 50.33443707399096, Max: 162, Min: 47\n",
      "Statistics for berty_tokens.berty_tokens-type-ids_length:\n",
      "\tLengths: Mean: 119.33333333333333, Standard Dev: 50.33443707399096, Max: 164, Min: 49\n",
      "Statistics for berty_tokens.mask_length:\n",
      "\tLengths: Mean: 117.33333333333333, Standard Dev: 50.33443707399096, Max: 162, Min: 47\n",
      "Statistics for berty_tokens.num_tokens:\n",
      "\tLengths: Mean: 119.33333333333333, Standard Dev: 50.33443707399096, Max: 164, Min: 49\n",
      "\n",
      "10 Random instances: \n",
      "Instance 1:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 162 with text: \n",
      " \t\t[cairo, is, now, home, to, some, 15, million, people, -, a, bu, ##rgeon, ##ing, population, that,\n",
      "\t\tproduces, approximately, 10, ,, 000, tonnes, of, rubbish, per, day, ,, putting, an, enormous,\n",
      "\t\tstrain, on, public, services, ., in, the, past, 10, years, ,, the, government, has, tried, hard, to,\n",
      "\t\tencourage, private, investment, in, the, refuse, sector, ,, but, some, estimate, 4, ,, 000, tonnes,\n",
      "\t\tof, waste, is, left, behind, every, day, ,, fest, ##ering, in, the, heat, as, it, waits, for,\n",
      "\t\tsomeone, to, clear, it, up, ., it, is, often, the, people, in, the, poor, ##est, neighbourhoods,\n",
      "\t\tthat, are, worst, affected, ., but, in, some, areas, they, are, fighting, back, ., in, shu, ##bra,\n",
      "\t\t,, one, of, the, northern, districts, of, the, city, ,, the, residents, have, taken, to, the,\n",
      "\t\tstreets, armed, with, dust, ##pan, ##s, and, brushes, to, clean, up, public, areas, which, have,\n",
      "\t\tbeen, used, as, public, dump, ##s, ., [SEP], 15, million, tonnes, of, rubbish, are, produced, daily,\n",
      "\t\tin, cairo, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 0:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 47 with text: \n",
      " \t\t[yet, ,, we, now, are, discovering, that, antibiotics, are, losing, their, effectiveness, against,\n",
      "\t\tillness, ., disease, -, causing, bacteria, are, mu, ##tat, ##ing, faster, than, we, can, come, up,\n",
      "\t\twith, new, antibiotics, to, fight, the, new, variations, ., [SEP], bacteria, is, winning, the, war,\n",
      "\t\tagainst, antibiotics, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 1:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 162 with text: \n",
      " \t\t[cairo, is, now, home, to, some, 15, million, people, -, a, bu, ##rgeon, ##ing, population, that,\n",
      "\t\tproduces, approximately, 10, ,, 000, tonnes, of, rubbish, per, day, ,, putting, an, enormous,\n",
      "\t\tstrain, on, public, services, ., in, the, past, 10, years, ,, the, government, has, tried, hard, to,\n",
      "\t\tencourage, private, investment, in, the, refuse, sector, ,, but, some, estimate, 4, ,, 000, tonnes,\n",
      "\t\tof, waste, is, left, behind, every, day, ,, fest, ##ering, in, the, heat, as, it, waits, for,\n",
      "\t\tsomeone, to, clear, it, up, ., it, is, often, the, people, in, the, poor, ##est, neighbourhoods,\n",
      "\t\tthat, are, worst, affected, ., but, in, some, areas, they, are, fighting, back, ., in, shu, ##bra,\n",
      "\t\t,, one, of, the, northern, districts, of, the, city, ,, the, residents, have, taken, to, the,\n",
      "\t\tstreets, armed, with, dust, ##pan, ##s, and, brushes, to, clean, up, public, areas, which, have,\n",
      "\t\tbeen, used, as, public, dump, ##s, ., [SEP], 15, million, tonnes, of, rubbish, are, produced, daily,\n",
      "\t\tin, cairo, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 0:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 47 with text: \n",
      " \t\t[yet, ,, we, now, are, discovering, that, antibiotics, are, losing, their, effectiveness, against,\n",
      "\t\tillness, ., disease, -, causing, bacteria, are, mu, ##tat, ##ing, faster, than, we, can, come, up,\n",
      "\t\twith, new, antibiotics, to, fight, the, new, variations, ., [SEP], bacteria, is, winning, the, war,\n",
      "\t\tagainst, antibiotics, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 0:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 47 with text: \n",
      " \t\t[yet, ,, we, now, are, discovering, that, antibiotics, are, losing, their, effectiveness, against,\n",
      "\t\tillness, ., disease, -, causing, bacteria, are, mu, ##tat, ##ing, faster, than, we, can, come, up,\n",
      "\t\twith, new, antibiotics, to, fight, the, new, variations, ., [SEP], bacteria, is, winning, the, war,\n",
      "\t\tagainst, antibiotics, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 2:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 143 with text: \n",
      " \t\t[the, ami, ##sh, community, in, pennsylvania, ,, which, numbers, about, 55, ,, 000, ,, lives, an,\n",
      "\t\tagrarian, lifestyle, ,, shu, ##nni, ##ng, technological, advances, like, electricity, and,\n",
      "\t\tautomobiles, ., and, many, say, their, ins, ##ular, lifestyle, gives, them, a, sense, that, they,\n",
      "\t\tare, protected, from, the, violence, of, american, society, ., but, as, residents, gathered, near,\n",
      "\t\tthe, school, ,, some, wearing, traditional, ga, ##rb, and, arriving, in, horse, -, drawn, bug,\n",
      "\t\t##gies, ,, they, said, that, sense, of, safety, had, been, shattered, ., \", if, someone, snaps, and,\n",
      "\t\twants, to, do, something, stupid, ,, there, ', s, no, distance, that, ', s, going, to, stop, them,\n",
      "\t\t,, \", said, jake, king, ,, 56, ,, an, ami, ##sh, lantern, maker, who, knew, several, families,\n",
      "\t\twhose, children, had, been, shot, ., [SEP], pennsylvania, has, the, biggest, ami, ##sh, community,\n",
      "\t\tin, the, u, ., s, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 1:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 162 with text: \n",
      " \t\t[cairo, is, now, home, to, some, 15, million, people, -, a, bu, ##rgeon, ##ing, population, that,\n",
      "\t\tproduces, approximately, 10, ,, 000, tonnes, of, rubbish, per, day, ,, putting, an, enormous,\n",
      "\t\tstrain, on, public, services, ., in, the, past, 10, years, ,, the, government, has, tried, hard, to,\n",
      "\t\tencourage, private, investment, in, the, refuse, sector, ,, but, some, estimate, 4, ,, 000, tonnes,\n",
      "\t\tof, waste, is, left, behind, every, day, ,, fest, ##ering, in, the, heat, as, it, waits, for,\n",
      "\t\tsomeone, to, clear, it, up, ., it, is, often, the, people, in, the, poor, ##est, neighbourhoods,\n",
      "\t\tthat, are, worst, affected, ., but, in, some, areas, they, are, fighting, back, ., in, shu, ##bra,\n",
      "\t\t,, one, of, the, northern, districts, of, the, city, ,, the, residents, have, taken, to, the,\n",
      "\t\tstreets, armed, with, dust, ##pan, ##s, and, brushes, to, clean, up, public, areas, which, have,\n",
      "\t\tbeen, used, as, public, dump, ##s, ., [SEP], 15, million, tonnes, of, rubbish, are, produced, daily,\n",
      "\t\tin, cairo, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 1:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 162 with text: \n",
      " \t\t[cairo, is, now, home, to, some, 15, million, people, -, a, bu, ##rgeon, ##ing, population, that,\n",
      "\t\tproduces, approximately, 10, ,, 000, tonnes, of, rubbish, per, day, ,, putting, an, enormous,\n",
      "\t\tstrain, on, public, services, ., in, the, past, 10, years, ,, the, government, has, tried, hard, to,\n",
      "\t\tencourage, private, investment, in, the, refuse, sector, ,, but, some, estimate, 4, ,, 000, tonnes,\n",
      "\t\tof, waste, is, left, behind, every, day, ,, fest, ##ering, in, the, heat, as, it, waits, for,\n",
      "\t\tsomeone, to, clear, it, up, ., it, is, often, the, people, in, the, poor, ##est, neighbourhoods,\n",
      "\t\tthat, are, worst, affected, ., but, in, some, areas, they, are, fighting, back, ., in, shu, ##bra,\n",
      "\t\t,, one, of, the, northern, districts, of, the, city, ,, the, residents, have, taken, to, the,\n",
      "\t\tstreets, armed, with, dust, ##pan, ##s, and, brushes, to, clean, up, public, areas, which, have,\n",
      "\t\tbeen, used, as, public, dump, ##s, ., [SEP], 15, million, tonnes, of, rubbish, are, produced, daily,\n",
      "\t\tin, cairo, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 1:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 162 with text: \n",
      " \t\t[cairo, is, now, home, to, some, 15, million, people, -, a, bu, ##rgeon, ##ing, population, that,\n",
      "\t\tproduces, approximately, 10, ,, 000, tonnes, of, rubbish, per, day, ,, putting, an, enormous,\n",
      "\t\tstrain, on, public, services, ., in, the, past, 10, years, ,, the, government, has, tried, hard, to,\n",
      "\t\tencourage, private, investment, in, the, refuse, sector, ,, but, some, estimate, 4, ,, 000, tonnes,\n",
      "\t\tof, waste, is, left, behind, every, day, ,, fest, ##ering, in, the, heat, as, it, waits, for,\n",
      "\t\tsomeone, to, clear, it, up, ., it, is, often, the, people, in, the, poor, ##est, neighbourhoods,\n",
      "\t\tthat, are, worst, affected, ., but, in, some, areas, they, are, fighting, back, ., in, shu, ##bra,\n",
      "\t\t,, one, of, the, northern, districts, of, the, city, ,, the, residents, have, taken, to, the,\n",
      "\t\tstreets, armed, with, dust, ##pan, ##s, and, brushes, to, clean, up, public, areas, which, have,\n",
      "\t\tbeen, used, as, public, dump, ##s, ., [SEP], 15, million, tonnes, of, rubbish, are, produced, daily,\n",
      "\t\tin, cairo, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n",
      "Instance 1:\n",
      "\tInstance with fields:\n",
      " \t berty_tokens: TextField of length 162 with text: \n",
      " \t\t[cairo, is, now, home, to, some, 15, million, people, -, a, bu, ##rgeon, ##ing, population, that,\n",
      "\t\tproduces, approximately, 10, ,, 000, tonnes, of, rubbish, per, day, ,, putting, an, enormous,\n",
      "\t\tstrain, on, public, services, ., in, the, past, 10, years, ,, the, government, has, tried, hard, to,\n",
      "\t\tencourage, private, investment, in, the, refuse, sector, ,, but, some, estimate, 4, ,, 000, tonnes,\n",
      "\t\tof, waste, is, left, behind, every, day, ,, fest, ##ering, in, the, heat, as, it, waits, for,\n",
      "\t\tsomeone, to, clear, it, up, ., it, is, often, the, people, in, the, poor, ##est, neighbourhoods,\n",
      "\t\tthat, are, worst, affected, ., but, in, some, areas, they, are, fighting, back, ., in, shu, ##bra,\n",
      "\t\t,, one, of, the, northern, districts, of, the, city, ,, the, residents, have, taken, to, the,\n",
      "\t\tstreets, armed, with, dust, ##pan, ##s, and, brushes, to, clean, up, public, areas, which, have,\n",
      "\t\tbeen, used, as, public, dump, ##s, ., [SEP], 15, million, tonnes, of, rubbish, are, produced, daily,\n",
      "\t\tin, cairo, .]\n",
      " \t\tand TokenIndexers : {'berty_tokens': 'PretrainedBertIndexer'} \n",
      " \t label: LabelField with label: not_entailment in namespace: 'labels'.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'berty_tokens': {'berty_tokens': tensor([[  101,  2664,  1010,  2057,  2085,  2024, 13648,  2008, 24479,  2024,\n",
       "            3974,  2037, 12353,  2114,  7355,  1012,  4295,  1011,  4786, 10327,\n",
       "            2024, 14163, 29336,  2075,  5514,  2084,  2057,  2064,  2272,  2039,\n",
       "            2007,  2047, 24479,  2000,  2954,  1996,  2047,  8358,  1012,   102,\n",
       "           10327,  2003,  3045,  1996,  2162,  2114, 24479,  1012,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101, 11096,  2003,  2085,  2188,  2000,  2070,  2321,  2454,  2111,\n",
       "            1011,  1037, 20934, 28242,  2075,  2313,  2008,  7137,  3155,  2184,\n",
       "            1010,  2199, 11000,  1997, 29132,  2566,  2154,  1010,  5128,  2019,\n",
       "            8216, 10178,  2006,  2270,  2578,  1012,  1999,  1996,  2627,  2184,\n",
       "            2086,  1010,  1996,  2231,  2038,  2699,  2524,  2000,  8627,  2797,\n",
       "            5211,  1999,  1996, 10214,  4753,  1010,  2021,  2070, 10197,  1018,\n",
       "            1010,  2199, 11000,  1997,  5949,  2003,  2187,  2369,  2296,  2154,\n",
       "            1010, 17037,  7999,  1999,  1996,  3684,  2004,  2009, 18074,  2005,\n",
       "            2619,  2000,  3154,  2009,  2039,  1012,  2009,  2003,  2411,  1996,\n",
       "            2111,  1999,  1996,  3532,  4355, 27535,  2008,  2024,  5409,  5360,\n",
       "            1012,  2021,  1999,  2070,  2752,  2027,  2024,  3554,  2067,  1012,\n",
       "            1999, 18454, 10024,  1010,  2028,  1997,  1996,  2642,  4733,  1997,\n",
       "            1996,  2103,  1010,  1996,  3901,  2031,  2579,  2000,  1996,  4534,\n",
       "            4273,  2007,  6497,  9739,  2015,  1998, 22569,  2000,  4550,  2039,\n",
       "            2270,  2752,  2029,  2031,  2042,  2109,  2004,  2270, 15653,  2015,\n",
       "            1012,   102,  2321,  2454, 11000,  1997, 29132,  2024,  2550,  3679,\n",
       "            1999, 11096,  1012,   102],\n",
       "          [  101,  1996, 26445,  4095,  2451,  1999,  3552,  1010,  2029,  3616,\n",
       "            2055,  4583,  1010,  2199,  1010,  3268,  2019, 23226,  9580,  1010,\n",
       "           18454, 23500,  3070, 10660,  9849,  2066,  6451,  1998, 19207,  1012,\n",
       "            1998,  2116,  2360,  2037, 16021,  7934,  9580,  3957,  2068,  1037,\n",
       "            3168,  2008,  2027,  2024,  5123,  2013,  1996,  4808,  1997,  2137,\n",
       "            2554,  1012,  2021,  2004,  3901,  5935,  2379,  1996,  2082,  1010,\n",
       "            2070,  4147,  3151, 11721, 15185,  1998,  7194,  1999,  3586,  1011,\n",
       "            4567, 11829, 17252,  1010,  2027,  2056,  2008,  3168,  1997,  3808,\n",
       "            2018,  2042, 10909,  1012,  1000,  2065,  2619, 20057,  1998,  4122,\n",
       "            2000,  2079,  2242,  5236,  1010,  2045,  1005,  1055,  2053,  3292,\n",
       "            2008,  1005,  1055,  2183,  2000,  2644,  2068,  1010,  1000,  2056,\n",
       "            5180,  2332,  1010,  5179,  1010,  2019, 26445,  4095, 12856,  9338,\n",
       "            2040,  2354,  2195,  2945,  3005,  2336,  2018,  2042,  2915,  1012,\n",
       "             102,  3552,  2038,  1996,  5221, 26445,  4095,  2451,  1999,  1996,\n",
       "            1057,  1012,  1055,  1012,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]]),\n",
       "  'berty_tokens-offsets': tensor([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "            15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "            29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "            43,  44,  45,  46,  47,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "            15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "            29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "            43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "            57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "            71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "            85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "            99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "           113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "           127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "           141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "           155, 156, 157, 158, 159, 160, 161, 162],\n",
       "          [  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "            15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "            29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "            43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "            57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "            71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "            85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "            99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "           113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "           127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "           141, 142, 143,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0]]),\n",
       "  'berty_tokens-type-ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'label': tensor([0, 1, 1])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch.print_statistics()\n",
    "batch.as_tensor_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
