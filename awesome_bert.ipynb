{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 09:35:21.231179 140631251642176 file_utils.py:38] PyTorch version 1.4.0 available.\n",
      "I0226 09:35:24.576916 140631251642176 filelock.py:274] Lock 140631136788832 acquired on /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "I0226 09:35:24.580226 140631251642176 file_utils.py:413] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /disks/sdb/torch_home/transformers/tmpd0z910re\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3993af42f0e4591bb589109e2d01848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 09:35:25.907350 140631251642176 file_utils.py:423] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 09:35:25.908533 140631251642176 file_utils.py:426] creating metadata file for /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 09:35:25.910006 140631251642176 filelock.py:318] Lock 140631136788832 released on /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "I0226 09:35:25.911094 140631251642176 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 09:35:25.912204 140631251642176 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 09:35:27.153762 140631251642176 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /disks/sdb/torch_home/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0226 09:35:28.613382 140631251642176 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /disks/sdb/torch_home/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 09:35:28.615016 140631251642176 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0226 09:35:29.979126 140631251642176 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /disks/sdb/torch_home/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0226 09:35:33.390035 140631251642176 modeling_utils.py:543] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0226 09:35:33.393536 140631251642176 modeling_utils.py:549] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help lower our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help minimize our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = '/disks/sdb/torch_home'\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAugmentor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def augment():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2047, 2101, 2060, 2048, 10947], [6409, 3891, 3465, 5366, 4254], [1012, 1025, 1029, 999, 1010]], [[2393, 2022, 2079, 5335, 2031], [6409, 3891, 3465, 5366, 4254], [1012, 1025, 1029, 999, 1010]]]\n",
      "[2047]\n",
      "[6409]\n",
      "[1012]\n",
      "[2393]\n",
      "[5366]\n",
      "[1012]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the new versions would help minimize our . losses',\n",
       " 'the large versions would costs minimize our . help']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "sent = \"the large versions would help minimize our carbon footprint.\"\n",
    "aug_num = 2\n",
    "change_num = 3\n",
    "top_k = 5\n",
    "temprature = 1.0\n",
    "stopwords = ['a', 'the', 'our', ',', '.', '?']\n",
    "\n",
    "sent_split = sent.split(\" \")\n",
    "\n",
    "allow_sids = []\n",
    "for i in range(len(sent_split)):\n",
    "    if sent_split[i] not in stopwords:\n",
    "        allow_sids.append(i)\n",
    "\n",
    "change_num = min(change_num, len(allow_sids))\n",
    "change_sids_lst = []\n",
    "inputs = []\n",
    "sent_splits = []\n",
    "for i in range(aug_num):\n",
    "    sids = random.sample(allow_sids, k=change_num)\n",
    "#     sids = [0, 2, 4]\n",
    "    change_sids_lst.append(sids)\n",
    "    tmp_sent_split = deepcopy(sent_split)\n",
    "    sent_splits.append(tmp_sent_split)\n",
    "    for sid in sids:\n",
    "        tmp_sent_split[sid] = tokenizer.mask_token\n",
    "    inputs.append(tokenizer.encode(tmp_sent_split, return_tensors=\"pt\"))\n",
    "inputs = torch.cat(inputs)\n",
    "\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "with torch.no_grad():\n",
    "    token_logits = model(inputs)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_k_logits, top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1)\n",
    "\n",
    "top_k_logits = top_k_logits.view(aug_num, change_num, top_k) / temprature\n",
    "top_k_probs = torch.softmax(top_k_logits, dim=-1).cpu().numpy()\n",
    "top_k_tokens = top_k_tokens.view(aug_num, change_num, top_k).cpu().numpy().tolist()\n",
    "\n",
    "print(top_k_tokens)\n",
    "\n",
    "for i in range(aug_num):\n",
    "    for j in range(change_num):\n",
    "        changed = np.random.choice(top_k_tokens[i][j], 1, p=top_k_probs[i][j])\n",
    "        print(changed)\n",
    "#         print(sent_splits[i][j])\n",
    "        sent_splits[i][change_sids_lst[i][j]] = tokenizer.decode([changed])\n",
    "\n",
    "sents = list(map(lambda x: \" \".join(x), sent_splits))\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the large versions would help reduce our carbon footprint.\n",
      " of the large versions would help lower our carbon footprint.\n",
      " of the large versions would help minimize our carbon footprint.\n",
      " of the large versions would help increase our carbon footprint.\n",
      " of the large versions would help decrease our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "sequence = \" of the large versions would help minimize our carbon footprint.\"\n",
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = model(inputs)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2008])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(top_k_tokens[0][0], 1, p=top_k_probs[0][0])\n",
    "# top_k_probs[0][0]\n",
    "# help(np.random.choice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
